# base
caption_model: m2transformer_local
input_json: data/mimicrmtalk.json
input_att_dir: data/mimicrm_att
input_fc_dir: data/mimicrm_fc
input_label_h5: data/mimicrmtalk_label.h5
learning_rate: 0.0005
#learning_rate_decay_start: 0
scheduled_sampling_start: 0
#checkpoint_path: logs/m2v11
# checkpoint_path: logs/log_fds_tf
# $start_from
language_eval: 0
checkpoint_path: logs/mimicrmm2_v2
#save_checkpoint_every: 3000
save_every_epoch: True
# val_images_use: -1
save_checkpoint_every: 270790
val_images_use: 2000
# checkpoint_path: logs/mimicrm_m2

save_every_epoch: True
save_history_ckpt: True
train_sample_n: 5
self_critical_after: -1
batch_size: 10
learning_rate_decay_start: 0
max_epochs: 30
max_length: 50