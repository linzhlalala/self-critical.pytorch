# base
caption_model: m2transformer_local
input_json: data/mimictalk.json
input_att_dir: data/mimictalk_att
input_fc_dir: data/mimictalk_fc
input_label_h5: data/mimictalk_label.h5
learning_rate: 0.0005
#learning_rate_decay_start: 0
scheduled_sampling_start: 0
#checkpoint_path: logs/m2v11
# checkpoint_path: logs/log_fds_tf
# $start_from
language_eval: 0
#save_checkpoint_every: 3000
save_every_epoch: True
val_images_use: -1
save_checkpoint_every: 54317
val_images_use: 15478
checkpoint_path: logs/mimic_m2

save_every_epoch: True
save_history_ckpt: True
train_sample_n: 5
self_critical_after: -1
batch_size: 10
learning_rate_decay_start: 0
max_epochs: 30
max_length: 50